---
jupyter:
  jupytext:
    formats: ipynb,md
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.16.4
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

<!-- #region editable=true slideshow={"slide_type": ""} tags=["title"] -->
# Time: the Cost of Reproducibility
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": ""} tags=["contributor"] -->
### Frédéric Clavert [![orcid](https://orcid.org/sites/default/files/images/orcid_16x16.png)](https://orcid.org/0000-0002-0237-2532) 
C<sup>2</sup>DH, University of Luxembourg
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": ""} tags=["contributor"] -->
### Elisabeth Guerard [![orcid](https://orcid.org/sites/default/files/images/orcid_16x16.png)](https://orcid.org/0000-0001-7742-4141) 
C<sup>2</sup>DH, University of Luxembourg
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": ""} tags=["contributor"] -->
### Andreas Fickers [![orcid](https://orcid.org/sites/default/files/images/orcid_16x16.png)](https://orcid.org/0000-0001-7742-4141) 
C<sup>2</sup>DH, University of Luxembourg
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": ""} tags=["copyright"] -->
[![cc-by](https://licensebuttons.net/l/by/4.0/88x31.png)](https://creativecommons.org/licenses/by/4.0/) 
©Frédéric Clavert, Élisabeth Guérard, Andreas Fickers. Published by De Gruyter in cooperation with the University of Luxembourg Centre for Contemporary and Digital History. This is an Open Access article distributed under the terms of the [Creative Commons Attribution License CC-BY](https://creativecommons.org/licenses/by/4.0/)

<!-- #endregion -->

```python editable=true slideshow={"slide_type": ""} tags=["cover"]
from IPython.display import Image, display
display(Image("./media/placeholder.png"))
```

<!-- #region editable=true slideshow={"slide_type": ""} tags=["disclaimer"] -->
*The cover image was generated by Microsoft Copilot (DALL·E 3)*. Most of the code of this article has been written with the help of Anthropic's Claude chatbot.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": ""} -->
### Abstract
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": ""} tags=["keywords"] -->
FirstKeyword, SecondKeyword, AlwaysSeparatedByAComma
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": ""} tags=["abstract"] -->
This is an abstract (...)
<!-- #endregion -->

<!-- #region tags=["narrative"] -->
## Introduction
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": ""} tags=["narrative"] -->
In a previous [editorial](https://journalofdigitalhistory.org/en/article/m7DWqDjY3hoV), we have tried to reflect on “what it means to sustain a digital project such as the *Journal of Digital History* within our current digital environment, which is unstable by nature”, while referring to the notion of *updatism*. One of the ways, as scholars, to face this unstable digital environment is to ensure, the best we can, the reproducibility of all articles that we publish.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": ""} tags=["narrative"] -->
In our journal we put strong emphasis on reproducibility, enabling readers to use the original data to regenerate the same results. To ensure the reproducibility specifically of the articles' code, the JDH's team has defined a thorough technical review that allows all published articles to be run on our reader's computer or to be run by services like MyBinder or Google Colab with a single click.
<!-- #endregion -->

However, last year we had quite a problem with delivering a stable output of regular article publications. Although we published articles on our platform in 2023, we could not open any new issue. Preparing three special issues at the same time -- which might have been a mistake but the quality of the articles we are receiving makes us happy to have accepted three special issues at the same time as well as an open-ended issue -- has been a challenge.Luckily, we were able to start publishing articles within the Digital Tools special issue a few weeks ago. This Varia issue -- one-year long issue for articles that are not related to any special issue -- is being published within the next few weeks and the DH China issue should follow soon.


Though we are very proud of the work we are achieving, all the more that, beyond evaluating and publishing issues and articles of, we think, excellent quality, we are also developing a publishing platform that could be adapted to other journals' needs. Nevertheless, after 4 years of development, and almost 3 years of editorial life, we can say that ensuring reproducibility comes at a cost: time.


## The lifespan of a *Journal of Digital History* article


Our publishing workflow can be described in several steps:

- **abstract submission** -- We asked all potential authors to submit an abstract proposal. This will be the occasion to meet with a member of the editorial team, to ensure that the article's project will fit the *Journal of Digital History* specificities, but also that those specificities will be of interest to the potential authors. This step also ensures a rather good quality control, explaining that for now, no finalised articles has been rejected yet;
- **writing** -- Once the abstract is accepted, authors will write their article. This step can take as long as the authors wish, unless they are publishing in a special issue, and even in the case of a special issue, as we are not publishing all articles at the same time, we can be flexible. We are conscious that writing an article for the JDH is challenging. From the beginning of this journey, installing the necessary writing environment, made of, among other pieces of software, jupyter notebooks, zotero, github, has been sometimes hard for authors. Nevertheless, since 2020, we also have been in touch, through workshops, seminars, conferences such as DH2022, DH Benelux or DH2024, with our authors to try making things easier. Authors have now at their disposal a rather easy way to install this writing environment through Docker to set up jupyter lab installation based on R or Python depending on their needs, with step-by-step instructions and tutorials. In order to also automate our process and provide best practices in terms of folder organization, we provide a GitHub template. And, last but not least, we offer one-to-one sessions for support in this set-up. We try to be available for our authors in case they encounter problems.
- **article submission** --  The submission of the article takes the form of an email, sent to the managing editor, with the URL of a GitHub repository. All articles of the *Journal of Digital History* are stored for now on GitHub.
- **technical review** -- Once the article is submitted, it will be reviewed mostly by Élisabeth Guérard, helped by a student-assistant and sometimes by the managing editor. This step can be long. As you will see below, it can take up to around 280 days in some (extreme) cases, for instance the first article we received with code written in R. We obviously lacked at that time the know-how to assess technically a notebook in R, but R's sensitivity to its library versions is another explanation. Experience matters, and we are now far better in dealing with articles using R. But new challenging cases are regularly popping out.
- **peer review** -- Ensuring high quality articles requires a thorough peer reviewing process. As all academic journal today, we face a situation where finding peer reviewers is hard and can take time. 
- **final preparation** -- this step is made of a final technical review, design review, the copy-editing of the article and the preparation of its advertisement on social media (today mainly Facebook and BlueSky). The design review can be and will be in the future a more and more challenging step, as it also concerns the readibility of tables and dataframes (including their labelling), the quality of figures and data-visualisations. One of the difficulties of the final step is the fact that each kind of final reviews (technical, copy-editing, design) have their own file formats to facilitate the process. The authors will have the occasion to perform a final check of their article before we advertise it. 


Let's now have a look at the duration of those different steps, from the article submission one (Figure 1 and 2).

<!-- #region tags=["hermeneutics"] -->
We had to reconstruct the lifespan of each of the 16 articles (we did not include editorials, nor the articles of this Varia issue) that were published until now by the *Journal of Digital History*. One of our problem is that we are using three different systems to deal with articles, which can sometimes be confusing: emails, manuscript central (ScholarOne) and our own backend. As a consequence, the [data that we are using here](JDH_review_chronology.csv) might contain mistakes. Furthermore, as we used another system for the first few months (Slack) to discuss with authors, some information is also there, though we are not using it anymore for more than one year now. This data has some limitations: the peer review duration encompasses different cases: articles that were accepted after one or two rounds of evaluation, as well as those that proved more difficult to evaluate. Furthermore, we are here using the data concerning only published articles. In other words, articles that will be published soon are more prone to have longer technical and peer reviews durations.
<!-- #endregion -->

```python editable=true slideshow={"slide_type": ""} tags=["hermeneutics"]
import pandas as pd
import plotly.graph_objects as go
from datetime import datetime

# Define a date parser function
def date_parser(date_string):
    return pd.to_datetime(date_string, format='%d/%m/%Y', errors='coerce')

# Read the CSV file with explicit date parsing
df = pd.read_csv('JDH_review_chronology.csv',
                 parse_dates=['Abstract submission', 'Article submission', 'Technical review begins', 'Peer review start', 'Peer review end', 'Publication Date'],
                 date_parser=date_parser)

# Calculate durations
df['Technical Review duration'] = (df['Peer review start'] - df['Article submission']).dt.days
df['Peer Review Duration'] = (df['Peer review end'] - df['Peer review start']).dt.days
df['Review to Publication'] = (df['Publication Date'] - df['Peer review end']).dt.days

# Calculate total duration
df['Total Duration'] = (df['Publication Date'] - df['Article submission']).dt.days

# Sort by Issue and then by Total Duration within each Issue
df = df.sort_values(['Issue', 'Total Duration'], ascending=[True, True])

# Shorten titles, create clickable links, and include issue information
df['Short Title'] = df['Title'].apply(lambda x: x[:40] + '...' if len(x) > 43 else x)
df['Clickable Title'] = df.apply(lambda row: f'<b>{row["Issue"]}:</b> <a href="{row["URL"]}" target="_blank">{row["Short Title"]}</a>', axis=1)

```

```python editable=true slideshow={"slide_type": ""} tags=["narrative"]
# Create the figure
fig = go.Figure()

# Define colors (reversed order to match the new stage order)
colors = ['#ff7f0e', '#2ca02c', '#d62728'][::-1]

# Create stacked bars (reversed order of stages)
stages = ['Technical Review duration', 'Peer Review Duration', 'Review to Publication']
for i, stage in enumerate(stages):
    fig.add_trace(go.Bar(
        y=df['Clickable Title'],
        x=df[stage],
        name=stage,
        orientation='h',
        marker=dict(color=colors[i]),
        hoverinfo='name+x',
        hovertemplate='%{y}<br>%{name}: %{x} days<extra></extra>'
    ))

# Update layout
fig.update_layout(
    barmode='stack',
    title='Article Submission and Review Process Duration (Sorted by Issue)',
    xaxis_title='Number of Days',
    yaxis_title='Issue: Article Title',
    height=1000,  # Increased height to accommodate all titles
    width=1200,
    legend=dict(
        orientation="h",
        yanchor="bottom",
        y=1.02,
        xanchor="right",
        x=1,
        traceorder='reversed'  # This ensures the legend items are in the same order as the stacked bars
    )
)

# Update y-axis to render HTML
fig.update_layout(yaxis={
    'tickmode': 'array',
    'tickvals': list(range(len(df))),
    'ticktext': df['Clickable Title'],
    'tickfont': {'size': 10},
})
# Show the plot
fig.show()
```

As you can see in Figure 1, the delays to publish an article with us can be quite long. The final part ('Review to publication') contains all the Article final preparation (last technical review, design review, copy-editing, social media advertisement preparation). There are quite some exceptions that would need explanation: the imagineRio article, for instance, has been publish almost two years after its submission, due to an editorial decision: submitted before the publication of the Digital Tools call for papers, whereas its peer review had already started, we decided, in agreement with the authors and the invited editors of the special issue that it would best fit in this issue.


Sometimes, an article's peer review can also be delayed due to unforeseen technical issues. *Dialects of Discord* has had a very long technical review as it was the first article in R that we published and we lacked some know-how to technically review it faster. Articles submitted in R today are just as normal as articles in Python for us today, though.

```python editable=true slideshow={"slide_type": ""} tags=["hermeneutics"]
import pandas as pd
import matplotlib.pyplot as plt

# Define a date parser function
def date_parser(date_string):
    return pd.to_datetime(date_string, format='%d/%m/%Y', errors='coerce')

# Read the CSV file with explicit date parsing
df = pd.read_csv('JDH_review_chronology.csv',
                 parse_dates=['Abstract submission', 'Article submission', 'Technical review begins', 'Peer review start', 'Peer review end', 'Publication Date'],
                 date_parser=date_parser)

# Calculate durations
df['Technical Review'] = (df['Peer review start'] - df['Article submission']).dt.days
df['Peer Review Duration'] = (df['Peer review end'] - df['Peer review start']).dt.days
df['Review to Publication'] = (df['Publication Date'] - df['Peer review end']).dt.days

# Print rows where 'Review to Publication' is negative
print("Rows with negative 'Review to Publication' duration:")
print(df[df['Review to Publication'] < 0][['Title', 'Issue', 'Peer review end', 'Publication Date', 'Review to Publication']])

# Group by issue and calculate mean durations
issue_means = df.groupby('Issue')[['Technical Review', 'Peer Review Duration', 'Review to Publication']].mean()

print("\nMean durations by issue:")
print(issue_means)

```

```python editable=true slideshow={"slide_type": ""} tags=["narrative"]
# Create a stacked bar plot
fig, ax = plt.subplots(figsize=(12, 6))
issue_means.plot(kind='bar', stacked=True, ax=ax)
plt.title('Mean Duration of Review Stages by Issue')
plt.xlabel('Issue')
plt.ylabel('Mean Duration (Days)')
plt.legend(title='Stage', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()

# Add value labels on the bars
for c in ax.containers:
    ax.bar_label(c, label_type='center')
```

Nevertheless, as you can see with both figure 1 & 2, the timespan between submission and publication has increased between our first issue and the two following ones. This can be explained in several ways. One of the first reasons is that we had less articles to deal with than we have now. At least 12 articles are still in peer review, *i.e.* two thirds of the number or articles we have already published.

<!-- #region editable=true slideshow={"slide_type": ""} tags=["hermeneutics"] -->
For more general statistics about the Journal of Digital History, [you can consult this page](https://journalofdigitalhistory.org/en/notebook-viewer/JTJGcHJveHktZ2l0aHVidXNlcmNvbnRlbnQlMkZDMkRIJTJGam91cm5hbC1vZi1kaWdpdGFsLWhpc3RvcnktYmFja2VuZCUyRmRldmVsb3AlMkZqZGhfc3RhdGlzdGljcy5pcHluYg==).
<!-- #endregion -->

## Identifying bottlenecks


So, for more than six months, we have tried to identify bottlenecks in our workflow:

- The peer review platform we were using, ScholarOne (Manuscript Central), lacks flexibility. As our partner De Gruyter has acquired Ubiquity Press, that is based on OJS, we decided to switch to OJS. But this switch is slow, as articles which started to be reviewed in the ScholarOne system will stay there until their review is over.
- As the managing editor, Frédéric Clavert, co-author of this editorial, was dealing alone with the peer review process, we asked two researchers to join us, Hannah Smyth (UCL) and Lorella Viola (VU Amsterdam), to join us.
- As our backend developer, Élisabeth Guérard, co-author of this editorial, was the only one to perform complex technical reviews, we decided to hire one student assistant to ease her workload and we plan to hire other student assistants in the future. When the switch to OJS will make the managing editor's workflow a bit lighter, he will also be able to help for technical reviews, as those are sometimes enlightening editorial questions.
- Last but not least, we switched from a double-blind to a single-blind peer review process. The double-blind peer review process led to the existence of competing versions of the articles, one stored on the authors' GitHub repository, and the other one on the JDH's GitHub account. This added complexity to an anonimysation process that is already complex for all journals.


Though those bottlenecks are specific to the JDH, we also had difficulties common to many other journals: recruiting reviewers has become hard. We had to ask up to 14 different colleagues for some articles for two reviews. Of course, we do not blame our colleagues here, we all had to refuse evaluating an article at one point in our career. But it enlightened the fact that we should remind our reader the hard work reviewers are doing.


## Sticking to reproducibility, respecting authors


When we started the *Journal of Digital History*, we knew we would face strong difficulties. We may have underestimated them. Our development method, that we can qualify as 'agile', may also prove insufficient, though it provided the required flexibility for a project that was in fact a double project: developing a publication platform and creating an academic journal. There were also some elements that we did not really foresee as they later happened. If you read our About Page, there isn't anything about reproducibility of what are executable papers. Reproducibility was implicit, but is today a major point of the JDH, and will stay a major point.


Nevertheless what is at stake with reproducibility is its cost to us and to our authors: time. In our epoch of ultra-fast circulation of information, even historians are pressed to work faster. And some of our authors have no choice, but to publish their results as soon as possible, including in digital history where, in some cases, code / algorithm can become obsolete in a few months.


It is up to us to find the best balance between the rigour that is required to publish an academic journal, our willingness to publish reproducible and executable papers and the need of our authors to publish in good conditions and to publish *timely*. To find the best balance, beyond identifying bottlenecks and finding solutions to them, we also need to communicate better with our authors, to set up more complete guidelines, which we will do in the next few months. 


But we also need to collectively think about the cost of reproducibility, time. As some authors are asking us to publish their date of submission, their date of acceptance and their date of publication together with their article, we have asked our editorial board which information about the technical / peer review processes we should provide to readers. The answers of our board members were not unanimous, to our surprise. If everybody agreed that being more transparent on our review process would be a strong advantage, some of our colleagues, and they obviously made a point, emphasised the fact that it could also increase the “publish or perish” pressure that we all face today, even in Humanities and Social Sciences.


<center>*</center>


A few weeks ago, our colleague Lars Wieneke left the C<sup>2</sup>DH to start a new phase of his career outside academia. As head of the centre's digital infrastructure, Lars was an enthusiastic member of the team which built the *Journal of Digital History*. We wish to thank him for his fantastic contribution to this project.
